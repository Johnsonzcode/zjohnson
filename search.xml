<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[During 12.22-12.24]]></title>
    <url>%2FPoetry%2FDuring-12.22-12.24.html</url>
    <content type="text"><![CDATA[路途所感瞥高山流水 噫仙雾缭绕于魔音灌耳 感思绪飞扬间始觉故里 房屋几座农家人余忆吾幼年 终日嬉戏无劳神他乡苦寒窗数载 师承先德苦栽培不知学子才尚浅 搁至此久得空闲绒雪纷飞不觉寒 道过中途迫遣返早知落雪差池生 何须改日把家还 var ap = new APlayer({ element: document.getElementById("aplayer-OtmRUlZO"), narrow: false, autoplay: false, showlrc: false, music: { title: "天分", author: "薛之谦", url: "http://www.ytmp3.cn/down/56232.mp3", pic: "", lrc: "" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap);]]></content>
      <categories>
        <category>Poetry</category>
      </categories>
      <tags>
        <tag>Sentiment</tag>
        <tag>Record</tag>
        <tag>Memory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Enrichment analysis of GO & KEGG]]></title>
    <url>%2FRNA-seq%2FGO-%26-KEGG.html</url>
    <content type="text"><![CDATA[前言GO是Gene ontology的缩写，是一系列用来描述基因、基因产物特性的语义（terms）。KEGG（ Kyoto Encyclopedia of Genes and Genomes），是一个系统分析基因产物在细胞中代谢途径的数据库，是一种最常用的代谢通路分析。 介绍GO是Gene ontology的缩写，是一系列用来描述基因、基因产物特性的语义（terms）。这些语义主要分为三种：细胞组份（Cellular Component，简称 GO-CC），用于描述基因产物在细胞中的位置，如内质网，核或蛋白酶体等；分子功能(Molecular Function，简称 GO-MF)，大部分指的是单个基因产物的功能，如结合活性或催化活性等。 生物学途径/过程 (biological process，简称 GO-BP)，多是指具有多个步骤的有序的生物过程，如细胞生长、分化和维持、凋亡以及信号传导等过程。 Pathway指代谢通路，对差异基因进行pathway分析，可以知道实验条件下哪些代谢通路发生显著改变。KEGG（ Kyoto Encyclopedia of Genes and Genomes），是一个系统分析基因产物在细胞中代谢途径的数据库，是一种最常用的代谢通路分析。 在线分析分析方式有很多，例如：神包 clusterProfiler，DAVID，KOBAS，webgestalt这里分别使用 KOBAS 和 webgestalt KOBASKOBAS 输入id：选择 id 类型、输入物种、输入id： 选择分析类型： 结果： 查看： 我点击通路名称时，得到404，可能是偶然因素 Webgestalt选择物种、分析方式、数据库、输入 ID 类型、参考序列 分析方式 ORA 或者 GSEA当选择数据库为 pathway，再选 KEGG 时，可做 KEGG 通路富集分析参考序列可自定义上传 参考 GO KEGG数据库用法 | 基因集功能注释 | 代谢通路富集 零基础的小白如何自己做GO/KEGG分析？ GO、KEGG富集分析——DAVID与KOBAS在线分析工具 推荐神包：clusterProfiler包做GO分析]]></content>
      <categories>
        <category>RNA-seq</category>
      </categories>
      <tags>
        <tag>Enrichment analysis</tag>
        <tag>KOBAS</tag>
        <tag>Webgestalt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Differential expression analysis -- DESeq2]]></title>
    <url>%2FRNA-seq%2FDESeq2.html</url>
    <content type="text"><![CDATA[前言DESeq2 是一款基于负二项分布的R包，对于 count data 进行处理，用于 RNA-seq 基因差异分析。 安装RGUI中输入：123if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;)BiocManager::install(&quot;DESeq2&quot;, version = &quot;3.8&quot;) 矩阵准备1. 载入表达矩阵表达矩阵的数据结构： gene 列为基因 ensemble_id，后几个列名为样本名称，中间为 count（次数） 2. 设置分组信息分组信息的结构： id 列为样本名称，后几列的列名为因素 核心步骤1.载入数据123456789101112131415161718192021222324252627options(stringsAsFactors = FALSE)# 载入 count 文件（由 htseq-count 计数，未标准化），sep 指定分隔符，col.names 指定列名# 一个因素：种类 bec 和 lyec，三个生物学重复ten &lt;- read.table(&quot;E:/RNA-SEQ/4.0deseq/E10_count.txt&quot;, sep=&quot;\t&quot;, col.names = c(&quot;gene_id&quot;,&quot;bec1&quot;))ele &lt;- read.table(&quot;E:/RNA-SEQ/4.0deseq/E11_count.txt&quot;, sep=&quot;\t&quot;, col.names = c(&quot;gene_id&quot;,&quot;bec2&quot;))fif &lt;- read.table(&quot;E:/RNA-SEQ/4.0deseq/E15_count.txt&quot;, sep=&quot;\t&quot;, col.names = c(&quot;gene_id&quot;,&quot;bec3&quot;))six &lt;- read.table(&quot;E:/RNA-SEQ/4.0deseq/E16_count.txt&quot;, sep=&quot;\t&quot;, col.names = c(&quot;gene_id&quot;,&quot;lyec1&quot;))eig &lt;- read.table(&quot;E:/RNA-SEQ/4.0deseq/E18_count.txt&quot;, sep=&quot;\t&quot;, col.names = c(&quot;gene_id&quot;,&quot;lyec2&quot;))thi &lt;- read.table(&quot;E:/RNA-SEQ/4.0deseq/E13_count.txt&quot;, sep=&quot;\t&quot;, col.names = c(&quot;gene_id&quot;,&quot;lyec3&quot;))# 合并各个表达矩阵，一次性合并报错了（估计是行数不统一问题）all &lt;- merge(ten, ele, by=&quot;gene_id&quot;)all &lt;- merge(all, fif, by=&quot;gene_id&quot;)all &lt;- merge(all, six, by=&quot;gene_id&quot;)all &lt;- merge(all, eig, by=&quot;gene_id&quot;)all &lt;- merge(all, thi, by=&quot;gene_id&quot;)# 把 ensemble_id 作为行号rownames(all) &lt;- all[,1]# 把 ensemble_id 列删除，因为已经由行号来表示all = all[,-1]# 删除整行 count 皆为零的行all = all[rowSums(all) &gt;0,]# 删除 count 文件最后五行的结果统计all = all[-1,]all = all[-1,]all = all[-1,]all = all[-1,]all = all[-1,] 设置分组信息1234# rep 指定生物学重复数condition &lt;- factor(c(rep(&quot;bec&quot;,3),rep(&quot;lyec&quot;,3)), levels = c(&quot;bec&quot;,&quot;lyec&quot;))# 创建 colDatacolData &lt;- data.frame(row.names=colnames(all), condition) 构建 dds（DESeqDataSet）12345# 载入包library(DESeq2)# 构建 ddsdds &lt;- DESeqDataSetFromMatrix(all, colData, design= ~ condition)dds &lt;- DESeq(dds) 结果123456789res= results(dds) # 或者 res = results(dds, contrast=c(&quot;condition&quot;, &quot;bec&quot;, &quot;lyec&quot;))# 根据 p-value 排序res = res[order(res$pvalue),]# 显示一共多少个 genes 上调和下调（FDR0.1）summary(res)# 保存为文件write.csv(res,file=&quot;All_results.csv&quot;)# padj小于0.05的 genestable(res$padj&lt;0.05) 提取差异基因1234# padj&lt;0.05 且 表达倍数取以2为对数后大于1或者小于-1的差异表达基因diff_gene_deseq2 &lt;-subset(res, padj &lt; 0.05 &amp; abs(log2FoldChange) &gt; 1)# 保存差异表达基因的结果write.csv(diff_gene_deseq2,file= &quot;DEGs_BEC_vs_LYEC.csv&quot;) gene symbol 注释123456789101112131415# 载入包，没安装的百度包名，进入 Bioconductor 安装library(&apos;biomaRt&apos;)library(&quot;curl&quot;)# 重新载入差异表达基因，head=T：将首行作为列名DEG &lt;- read.table(&quot;E:/RNA-SEQ/4.0deseq/DEGs_BEC_vs_LYEC.csv&quot;, sep=&quot;,&quot;, head = T)# 查看有哪些数据库可用ensembl=useMart(&quot;ensembl&quot;)# 查看需要用到的物种的数据集listDatasets(ensembl) # ggallus_gene_ensembl：鸡的数据集，数据库选择 ensemblemart &lt;- useDataset(&quot;ggallus_gene_ensembl&quot;, useMart(&quot;ensembl&quot;))# 将 id 赋值给 my_ensembl_gene_idmy_ensembl_gene_id&lt;-DEG[,1]# 注释结果gg_symbols&lt;- getBM(attributes=c(&apos;ensembl_gene_id&apos;,&apos;external_gene_name&apos;,&quot;description&quot;),filters = &apos;ensembl_gene_id&apos;, values = my_ensembl_gene_id, mart = mart) 合并差异基因和注释结果被合并的两个表要有共同的列和列名1234# 按 ensembl_gene_id 合并，如果表不大可用 Excel 插入共同的列和列名DEG_ann&lt;-merge(DEG,gg_symbols,by=&quot;ensembl_gene_id&quot;)# 将合并结果保存为文件write.csv(DEG_ann,file= &quot;DEGs_BEC_vs_LYEC_ANN.csv&quot;) 下游分析GO 和 KEGG 富集分析（也可以有蛋白网络互作分析） 参考 RNA-seq(7): DEseq2筛选差异表达基因并注释(bioMart) 简单使用DESeq2/EdgeR做差异分析 Bioconductor:DESeq2 RNA_seq分析流程软件思维导图]]></content>
      <categories>
        <category>RNA-seq</category>
      </categories>
      <tags>
        <tag>Differential expression analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gene expression levels analysis -- HTSeq-count]]></title>
    <url>%2FRNA-seq%2FHTSeq-count.html</url>
    <content type="text"><![CDATA[前言HTSeq 是一款针对于有参考基因组的，分析高通量测序数据的Python库。官网： HTSeq 安装HTSeq作为Python库推荐采用pip安装方式：1pip install htseq 还能采用源码安装、conda安装、apt-get等 使用1htseq-count [options] &lt;alignment_file&gt; &lt;gtf_file&gt; 这里的 alignment file 可以是 bam 或者 sam 文件 上游准备 已排序的比对文件： sam 或者 bam 文件 排序要求建议按 reads name 排序 计数模型：union, intersection-strict and intersection-nonempty 测序的建库工作是否为链特异性建库，一般不是 参数 type 和 idattr 的确定，非 Ensemble 的注释文件这两个参数尚待明确，知道的麻烦告知一下，提前感谢！下游适配HTSeq 的计数结果没有进行标准化，因为我下游分析是采用 DESeq2，不需要标准化常用参数1234567891011# 命令参数-f | --format default: sam 设置输入文件的格式，该参数的值可以是sam或bam。-r | --order default: name 设置sam或bam文件的排序方式，该参数的值可以是name或pos。前者表示按read名进行排序，后者表示按比对的参考基因组位置进行排序。若测序数据是双末端测序，当输入sam/bam文件是按pos方式排序的时候，两端reads的比对结果在sam/bam文件中一般不是紧邻的两行，程序会将reads对的第一个比对结果放入内存，直到读取到另一端read的比对结果。因此，选择pos可能会导致程序使用较多的内存，它也适合于未排序的sam/bam文件。而pos排序则表示程序认为双末端测序的reads比对结果在紧邻的两行上，也适合于单端测序的比对结果。很多其它表达量分析软件要求输入的sam/bam文件是按pos排序的，但HTSeq推荐使用name排序，且一般比对软件的默认输出结果也是按name进行排序的。-s | --stranded default: yes 设置是否是链特异性测序。该参数的值可以是yes,no或reverse。no表示非链特异性测序；若是单端测序，yes表示read比对到了基因的正义链上；若是双末端测序，yes表示read1比对到了基因正义链上，read2比对到基因负义链上；reverse表示双末端测序情况下与yes值相反的结果。根据说明文件的理解，一般情况下双末端链特异性测序，该参数的值应该选择reverse（本人暂时没有测试该参数）。-a | --a default: 10 忽略比对质量低于此值的比对结果。在0.5.4版本以前该参数默认值是0。-t | --type default: exon 程序会对该指定的feature（gtf/gff文件第三列）进行表达量计算，而gtf/gff文件中其它的feature都会被忽略。-i | --idattr default: gene_id 设置feature ID是由gtf/gff文件第9列那个标签决定的；若gtf/gff文件多行具有相同的feature ID，则它们来自同一个feature，程序会计算这些features的表达量之和赋给相应的feature ID。-m | --mode default: union 设置表达量计算模式。该参数的值可以有union, intersection-strict and intersection-nonempty。这三种模式的选择请见上面对这3种模式的示意图。从图中可知，对于原核生物，推荐使用intersection-strict模式；对于真核生物，推荐使用union模式。-o | --samout 输出一个sam文件，该sam文件的比对结果中多了一个XF标签，表示该read比对到了某个feature上。-q | --quiet 不输出程序运行的状态信息和警告信息。-h | --help 输出帮助信息。 批量化处理借助bash：123456789#!/bin/bashfor i in 10 11; do &#123; htseq-count -f bam -s no in/E$&#123;i&#125;.sorted.bam\ anno/Gallus_gallus.Gallus_gallus-5.0.94.gtf &gt; out/E$&#123;i&#125;_count.txt &#125;;donewait 此处参考基因组来自于 Ensemble，而 -t 和 -i 在采用 Ensemble 的注释文件时默认为：exon 和 gene_id 输出结果文件为：1234567891011121314151617181920212223head counts.txtENSG00000000003 0ENSG00000000005 0ENSG00000000419 1171ENSG00000000457 563ENSG00000000460 703ENSG00000000938 0ENSG00000000971 1ENSG00000001036 925ENSG00000001084 1468ENSG00000001167 2997 tail count.txtENSG00000283696 18ENSG00000283697 0ENSG00000283698 1ENSG00000283699 0ENSG00000283700 0__no_feature 3469791__ambiguous 630717__too_low_aQual 1346501__not_aligned 520623__alignment_not_unique 2849422 下游分析需要将其合并到一个文件中，一个行为基因名，列为样本名，中间为 count 的行列式文件。这里也可以将其中 count 为零的行删除。所以应该是先合并、再删除计数全为零的行。 参考 RNA-seq分析htseq-count的使用 RNA-seq流程分析比较之半小时得到差异基因 转录组入门(6)： reads计数 官网]]></content>
      <categories>
        <category>RNA-seq</category>
      </categories>
      <tags>
        <tag>reads count</tag>
        <tag>gene quantification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mapping -- HISAT2]]></title>
    <url>%2FRNA-seq%2FHISAT2.html</url>
    <content type="text"><![CDATA[前言针对一般人类群体（以及有参考基因组的群体），HISAT2是一种快速灵敏的比对程序，用于绘制第二代测序序列（全基因组，转录组和外显子组测序数据）。是有参转录组常用的比对软件（无参常用bowtie2），HISAT2是TopHat2/Bowtie2的继任者，使用改进的BWT算法，实现了更快的速度和更少的资源占用。官网：HISAT2 下载安装 源码编译 下载：Source code 解压 hisat2-2.1.0-source.zip 切换到解压目录 编译：目录下 make 将 HISAT2 加入环境变量 二进制文件 下载：Binary（Linux_x86_64） 解压 hisat2-2.1.0-Linux_x86_64.zip，包含下面用到的文件（hisat2、xx.py、hisat2-build等） 建立索引需要参考序列（注释文件）。先建立基因组、转录组、SNP的索引文件，再建立其索引（以下代码在终端中运行，注意你的参考基因组等文件存放位置可能与我不同）。 建立基因组的索引文件： 12cd hisat2-2.1.0-Linux_x86_64extract_exons.py Homo_sapiens.GRCh38.83.chr.gtf &gt; genome.exon 建立转录组的索引文件： 1extract_splice_sites.py Homo_sapiens.GRCh38.83.chr.gtf &gt; genome.ss 建立SNP的索引文件： 1extract_snps.py snp142Common.txt &gt; genome.snp 建立索引： 1hisat2-build -p 4 genome.fa --snp genome.snp --ss genome.ss --exon genome.exon genome_snp_tran -p 指定线程数，genome_snp_tran 为索引的名称在第四步时，如果没有前三步会自动建立起 exon、snp、splice sites 文件及其索引，但是提前建立会提高分析效率前三步在过程中会占用一定的磁盘空间，我的磁盘空间有所不足，所以我选择直接采用第四步，有条件的尽量。所以注释文件可不下载 比对1hisat2 -p 16 -x ./grch38_tran/genome_tran -1 SRR534293_1.fastq -2 SRR534293_2.fastq –S SRR534293.sam 注： -p 线程数。-1 fastq文件，-2 另一端fastq文件（支持gzip文件）。-S 输出的SAM文件。 结合samtoolsHISAT2通过管道符（|）与samtools结合，将HISAT2输出的SAM文件排序、转化为BAM文件，加速分析流程、节约硬盘空间12hisat2 -p 4 -x index/genome -1 in/out.E10_1.fq.gz -2 in/out.E10_2.fq.gz | samtools sort -n -o \out/E10.sorted.bam 批量化处理借助bash，批量化处理123456789#!/bin/bashfor i in 10 11; do &#123; hisat2-2.1.0/hisat2 -p 4 -x index/genome -1 in/out.E$&#123;i&#125;_1.fq.gz -2 in/out.E$&#123;i&#125;_2.fq.gz \ | samtools sort -n -o out/E$&#123;i&#125;.sorted.bam &#125;;donewait 参考 官网：HISAT2 比对软件hisat2的使用 HISAT2-StringTie-Ballgown有参转录组数据分析 RNA-seq(5):序列比对：Hisat2]]></content>
      <categories>
        <category>RNA-seq</category>
      </categories>
      <tags>
        <tag>Alignment</tag>
        <tag>Mapping</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Quality control -- Fastp]]></title>
    <url>%2FRNA-seq%2FFastp.html</url>
    <content type="text"><![CDATA[前言Fastp是一款旨在提供FastQ文件快速的一体化预处理工具。由C语言开发且支持多线程的高性能表现。官方文档：Fastp in GitHub文章链接：https://www.biorxiv.org/content/early/2018/03/01/274100 功能特点 对数据自动进行全方位质控，生成人性化的报告 过滤功能（低质量，太短，太多N……） 对每一个序列的头部或尾部，计算滑动窗内的质量均值，并将均值较低的子序列进行切除（类似Trimmomatic的做法，但是快非常多） 全局剪裁 （在头/尾部，不影响去重），对于Illumina下机数据往往最后一到两个cycle需要这样处理 去除接头污染。厉害的是，你不用输入接头序列，因为算法会自动识别接头序列并进行剪裁 对于双端测序（PE）的数据，软件会自动查找每一对read的重叠区域，并对该重叠区域中不匹配的碱基对进行校正 去除尾部的polyG。对于Illumina NextSeq/NovaSeq的测序数据，因为是两色法发光，polyG是常有的事，所以该特性对该两类测序平台默认打开 对于PE数据中的overlap区间中不一致的碱基对，依据质量值进行校正 可以对带分子标签（UMI）的数据进行预处理，不管UMI在插入片段还是在index上，都可以轻松处理 可以将输出进行分拆，而且支持两种模式，分别是指定分拆的个数，或者分拆后每个文件的行数 更多查看官方文档 注意： 默认已经开启一些功能，也可关闭或者改变mo某个功能的默认参数 支持gzip输入与输出 支持SE（single end）和PE（paired end）数据 完美支持short reads和一定程度的long reads 生成html和json报告，且可动态交互（html预览 json预览） 下载安装官方文档介绍了三种获得Fastp的方式： Install with Bioconda (may be not the lastest version) 1conda install -c bioconda fastp Download binary (only for Linux system, http://opengene.org/fastp/fastp)This binary was compiled on CentOS, and tested on CentOS/Ubuntu (测试于Ubuntu 18.04 &amp; 16.04可用) 12wget http://opengene.org/fastp/fastpchmod a+x ./fastp Compile from sourceGet source (you can also use browser to download from master or releases) 123456git clone https://github.com/OpenGene/fastp.git# buildcd fastpmake# Installsudo make install 我选择二进制文件，可直接按如下代码使用，无需安装1./fastp [option] 简单使用SE数据：1fastp -i in.fq -o out.fq 输入需要过滤、质控和预处理的fastq文件（in.fq）输出clean data（out.fq）、报告文件（fast.html和fastp.json）报告文件名称可通过 -h xx.html 和 -j xx.json 更改 PE数据：1fastp -i in.R1.fq -o out.R1.fq -I in.R2.fq -O out.R2.fq 注意： o与O是小写字母o与大写字母O gzip数据的输入无需参数，自动按后缀识别（xx.gz）例：1fastp -i in.R1.fq.gz -I in.R2.fq.gz -o out.R1.fq.gz -O out.R2.fq.gz 精确使用具体参数的使用、调节都需要根据具体的质控要求及项目或者文章结果要求进行更改。附上全部参数：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495usage: fastp -i &lt;in1&gt; -o &lt;out1&gt; [-I &lt;in1&gt; -O &lt;out2&gt;] [options...]options: # I/O options -i, --in1 read1 input file name (string) -o, --out1 read1 output file name (string [=]) -I, --in2 read2 input file name (string [=]) -O, --out2 read2 output file name (string [=]) -6, --phred64 indicate the input is using phred64 scoring (it&apos;ll be converted to phred33, so the output will still be phred33) -z, --compression compression level for gzip output (1 ~ 9). 1 is fastest, 9 is smallest, default is 4. (int [=4]) --stdin input from STDIN. If the STDIN is interleaved paired-end FASTQ, please also add --interleaved_in. --stdout output passing-filters reads to STDOUT. This option will result in interleaved FASTQ output for paired-end input. Disabled by default. --interleaved_in indicate that &lt;in1&gt; is an interleaved FASTQ which contains both read1 and read2. Disabled by default. --reads_to_process specify how many reads/pairs to be processed. Default 0 means process all reads. (int [=0]) --dont_overwrite don&apos;t overwrite existing files. Overwritting is allowed by default. # adapter trimming options -A, --disable_adapter_trimming adapter trimming is enabled by default. If this option is specified, adapter trimming is disabled -a, --adapter_sequence the adapter for read1. For SE data, if not specified, the adapter will be auto-detected. For PE data, this is used if R1/R2 are found not overlapped. (string [=auto]) --adapter_sequence_r2 the adapter for read2 (PE data only). This is used if R1/R2 are found not overlapped. If not specified, it will be the same as &lt;adapter_sequence&gt; (string [=]) --detect_adapter_for_pe by default, the adapter sequence auto-detection is enabled for SE data only, turn on this option to enable it for PE data. # global trimming options -f, --trim_front1 trimming how many bases in front for read1, default is 0 (int [=0]) -t, --trim_tail1 trimming how many bases in tail for read1, default is 0 (int [=0]) -b, --max_len1 if read1 is longer than max_len1, then trim read1 at its tail to make it as long as max_len1. Default 0 means no limitation (int [=0]) -F, --trim_front2 trimming how many bases in front for read2. If it&apos;s not specified, it will follow read1&apos;s settings (int [=0]) -T, --trim_tail2 trimming how many bases in tail for read2. If it&apos;s not specified, it will follow read1&apos;s settings (int [=0]) -B, --max_len2 if read2 is longer than max_len2, then trim read2 at its tail to make it as long as max_len2. Default 0 means no limitation. If it&apos;s not specified, it will follow read1&apos;s settings (int [=0]) # polyG tail trimming, useful for NextSeq/NovaSeq data -g, --trim_poly_g force polyG tail trimming, by default trimming is automatically enabled for Illumina NextSeq/NovaSeq data --poly_g_min_len the minimum length to detect polyG in the read tail. 10 by default. (int [=10]) -G, --disable_trim_poly_g disable polyG tail trimming, by default trimming is automatically enabled for Illumina NextSeq/NovaSeq data # polyX tail trimming -x, --trim_poly_x enable polyX trimming in 3&apos; ends. --poly_x_min_len the minimum length to detect polyX in the read tail. 10 by default. (int [=10]) # per read cutting by quality options -5, --cut_by_quality5 enable per read cutting by quality in front (5&apos;), default is disabled (WARNING: this will interfere deduplication for both PE/SE data) -3, --cut_by_quality3 enable per read cutting by quality in tail (3&apos;), default is disabled (WARNING: this will interfere deduplication for SE data) -W, --cut_window_size the size of the sliding window for sliding window trimming, default is 4 (int [=4]) -M, --cut_mean_quality the bases in the sliding window with mean quality below cutting_quality will be cut, default is Q20 (int [=20]) # quality filtering options -Q, --disable_quality_filtering quality filtering is enabled by default. If this option is specified, quality filtering is disabled -q, --qualified_quality_phred the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified. (int [=15]) -u, --unqualified_percent_limit how many percents of bases are allowed to be unqualified (0~100). Default 40 means 40% (int [=40]) -n, --n_base_limit if one read&apos;s number of N base is &gt;n_base_limit, then this read/pair is discarded. Default is 5 (int [=5]) # length filtering options -L, --disable_length_filtering length filtering is enabled by default. If this option is specified, length filtering is disabled -l, --length_required reads shorter than length_required will be discarded, default is 15. (int [=15]) --length_limit reads longer than length_limit will be discarded, default 0 means no limitation. (int [=0]) # low complexity filtering -y, --low_complexity_filter enable low complexity filter. The complexity is defined as the percentage of base that is different from its next base (base[i] != base[i+1]). -Y, --complexity_threshold the threshold for low complexity filter (0~100). Default is 30, which means 30% complexity is required. (int [=30]) # filter reads with unwanted indexes (to remove possible contamination) --filter_by_index1 specify a file contains a list of barcodes of index1 to be filtered out, one barcode per line (string [=]) --filter_by_index2 specify a file contains a list of barcodes of index2 to be filtered out, one barcode per line (string [=]) --filter_by_index_threshold the allowed difference of index barcode for index filtering, default 0 means completely identical. (int [=0]) # base correction by overlap analysis options -c, --correction enable base correction in overlapped regions (only for PE data), default is disabled --overlap_len_require the minimum length of the overlapped region for overlap analysis based adapter trimming and correction. 30 by default. (int [=30]) --overlap_diff_limit the maximum difference of the overlapped region for overlap analysis based adapter trimming and correction. 5 by default. (int [=5]) # UMI processing -U, --umi enable unique molecular identifier (UMI) preprocessing --umi_loc specify the location of UMI, can be (index1/index2/read1/read2/per_index/per_read, default is none (string [=]) --umi_len if the UMI is in read1/read2, its length should be provided (int [=0]) --umi_prefix if specified, an underline will be used to connect prefix and UMI (i.e. prefix=UMI, UMI=AATTCG, final=UMI_AATTCG). No prefix by default (string [=]) --umi_skip if the UMI is in read1/read2, fastp can skip several bases following UMI, default is 0 (int [=0]) # overrepresented sequence analysis -p, --overrepresentation_analysis enable overrepresented sequence analysis. -P, --overrepresentation_sampling One in (--overrepresentation_sampling) reads will be computed for overrepresentation analysis (1~10000), smaller is slower, default is 20. (int [=20]) # reporting options -j, --json the json format report file name (string [=fastp.json]) -h, --html the html format report file name (string [=fastp.html]) -R, --report_title should be quoted with &apos; or &quot;, default is &quot;fastp report&quot; (string [=fastp report]) # threading options -w, --thread worker thread number, default is 2 (int [=2]) # output splitting options -s, --split split output by limiting total split file number with this option (2~999), a sequential number prefix will be added to output name ( 0001.out.fq, 0002.out.fq...), disabled by default (int [=0]) -S, --split_by_lines split output by limiting lines of each file with this option(&gt;=1000), a sequential number prefix will be added to output name ( 0001.out.fq, 0002.out.fq...), disabled by default (long [=0]) -d, --split_prefix_digits the digits for the sequential number padding (1~10), default is 4, so the filename will be padded as 0001.xxx, 0 to disable padding (int [=4]) # help -?, --help print this message 我的项目的质控要求： 去除接头（对于双端已自动去除，无需参数） 去除含N比例大于0.1的 reads（参数 -n 的默认值为5，我的测序reads 长125bp，则 -n 应为12.5≈13） 去除低质量 reads（Q phred &lt;= 20 的碱基数占整个 read 长度的 50％以上的 reads）（-q 20 -u 50） 速度尽可能快：线程数：-w 4（CPU：双核四线程） 注意： 线程数根据CPU能力调节 质控要求所对应的参数调节具体查看官方文档 批量化处理数据时，可写出相应 bash 脚本：12345678for i in 10 11; do &#123; ./fastp -i raw/E$&#123;i&#125;_1.fq.gz -I raw/E$&#123;i&#125;_2.fq.gz\ -o out/out.E$&#123;i&#125;_1.fq.gz -O out/out.E$&#123;i&#125;_2.fq.gz\ -q 20 -u 50 -n 13 -w 4 -h out/E$&#123;i&#125;.html -j out/E$&#123;i&#125;.json &#125;;donewait 注意： 这里的 raw 和 out 是我创建的文件夹，便于管理 bash 语法自行学习使用 参考 官方文档 使用fastp进行数据质控 fastp: 一款超快速全功能的FASTQ文件自动化质控+过滤+校正+预处理软件]]></content>
      <categories>
        <category>RNA-seq</category>
      </categories>
      <tags>
        <tag>Fastp</tag>
        <tag>Quality control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2FHexo%2Fhello-world.html</url>
    <content type="text"><![CDATA[初始文章欢迎使用 Hexo! 这是我的第一篇文章. 看看文档 documentation 了解更多. 如果你在使用Hexo的过程中遇到了任何问题, 你能在 troubleshooting 寻找答案 或者你能在 GitHub 问我. 快速开始创建一篇新文章1$ hexo new "My New Post" 更多信息: Writing 启动服务器1$ hexo server 更多信息: Server 生成静态文件1$ hexo generate 更多信息: Generating 部署到远程站点1$ hexo deploy 更多信息: Deployment]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
